)
# Step 3: construct Time-dependent Cox model
cox_td_OS <- coxph(Surv(tstart, tstop, DTH) ~ has_TTFD10 +
ARM + EMPLOY + LESION1 + ECOG + HER2,
data = td_data)
# Step 4: summarize results
summary(cox_td_OS)
###CO
setwd("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD+Covariates")
library(data.table)
library(dplyr)
library(purrr)
###PF2
setwd("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/Functional_Scales")
PF2<-fread("Breast_PRO_Physical_functioning(revised)_22OCT2025.csv")
# Calculate TTFD10 and TTFD10P for each subject (UID)
ttfd_result <- PF2 %>%
group_by(UID) %>%
arrange(TIME) %>%
group_modify(~{
baseline <- first(.x$DV2)
# TTFD10
idx <- which(.x$DV2 >= baseline + 10)
TTFD10 <- if (length(idx) > 0) .x$TIME[idx[1]] else max(.x$TIME)
censored <- ifelse(length(idx) > 0, 0, 1)
# TTFD10P
delta <- .x$DV2 - baseline
ddiff <- diff(delta)
if (any(ddiff >= 10)) {
i <- which(ddiff >= 10)[1]
TTFD10P <- .x$TIME[i + 1]
censored10P <- 0
} else {
TTFD10P <- max(.x$TIME)
censored10P <- 1
}
tibble(
baseline = baseline,
TTFD10 = TTFD10,
censored = censored,
TTFD10P = TTFD10P,
censored10P = censored10P
)
}) %>% ungroup()
View(ttfd_result)
View(PF2)
View(ttfd_result)
library(readr)
TTFD_CF_19JUN2025 <- read_csv("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD/Functional_Scale/TTFD_CF_19JUN2025.csv")
View(TTFD_CF_19JUN2025)
library(readr)
TTFD_EF_19JUN2025 <- read_csv("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD/Functional_Scale/TTFD_EF_19JUN2025.csv")
View(TTFD_EF_19JUN2025)
library(readr)
TTFD_PF2_19JUN2025 <- read_csv("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD/Functional_Scale/TTFD_PF2_19JUN2025.csv")
View(TTFD_PF2_19JUN2025)
library(readr)
TTFD_RF2_19JUN2025 <- read_csv("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD/Functional_Scale/TTFD_RF2_19JUN2025.csv")
View(TTFD_RF2_19JUN2025)
library(readr)
TTFD_SF_24SEP2025 <- read_csv("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD/Functional_Scale/TTFD_SF_24SEP2025.csv")
View(TTFD_SF_24SEP2025)
library(readr)
TTFD_CF_22OCT2025 <- read_csv("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD/TTFD_CF_22OCT2025.csv")
View(TTFD_CF_22OCT2025)
library(readr)
TTFD_EF_22OCT2025 <- read_csv("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD/TTFD_EF_22OCT2025.csv")
View(TTFD_EF_22OCT2025)
library(readr)
TTFD_PF2_22OCT2025 <- read_csv("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD/TTFD_PF2_22OCT2025.csv")
View(TTFD_PF2_22OCT2025)
library(readr)
TTFD_RF2_22OCT2025 <- read_csv("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD/TTFD_RF2_22OCT2025.csv")
View(TTFD_RF2_22OCT2025)
library(readr)
TTFD_SF_22OCT2025 <- read_csv("~/Desktop/Zhou Lab/Breast Cancer PRO project/Analysis/wwanbing/output/TTFD/TTFD_SF_22OCT2025.csv")
View(TTFD_SF_22OCT2025)
colnames(TTFD_SF_22OCT2025)
View(TTFD_CF_19JUN2025)
View(TTFD_CF_22OCT2025)
compare_ttfd <- function(a, b, tag_a = "A", tag_b = "B") {
x <- a %>%
select(UID, TTFD10, censored, TTFD10P, censored10P) %>%
rename_with(~paste0(., "_", tag_a), -UID) %>%
full_join(
b %>%
select(UID, TTFD10, censored, TTFD10P, censored10P) %>%
rename_with(~paste0(., "_", tag_b), -UID),
by = "UID"
) %>%
mutate(
eq_TTFD10     = .data[[paste0("TTFD10_", tag_a)]]     == .data[[paste0("TTFD10_", tag_b)]]     |
(is.na(.data[[paste0("TTFD10_", tag_a)]])     & is.na(.data[[paste0("TTFD10_", tag_b)]])),
eq_censored   = .data[[paste0("censored_", tag_a)]]   == .data[[paste0("censored_", tag_b)]]   |
(is.na(.data[[paste0("censored_", tag_a)]])   & is.na(.data[[paste0("censored_", tag_b)]])),
eq_TTFD10P    = .data[[paste0("TTFD10P_", tag_a)]]    == .data[[paste0("TTFD10P_", tag_b)]]    |
(is.na(.data[[paste0("TTFD10P_", tag_a)]])    & is.na(.data[[paste0("TTFD10P_", tag_b)]])),
eq_censored10P= .data[[paste0("censored10P_", tag_a)]]== .data[[paste0("censored10P_", tag_b)]]|
(is.na(.data[[paste0("censored10P_", tag_a)]])& is.na(.data[[paste0("censored10P_", tag_b)]]))
)
summary <- summarise(x,
n_UID = n(),
TTFD10_match      = mean(eq_TTFD10,      na.rm = TRUE) * 100,
censored_match    = mean(eq_censored,    na.rm = TRUE) * 100,
TTFD10P_match     = mean(eq_TTFD10P,     na.rm = TRUE) * 100,
censored10P_match = mean(eq_censored10P, na.rm = TRUE) * 100
)
mismatch <- x %>%
filter(!eq_TTFD10 | !eq_censored | !eq_TTFD10P | !eq_censored10P) %>%
select(UID,
paste0("TTFD10_", tag_a),      paste0("TTFD10_", tag_b),
paste0("censored_", tag_a),    paste0("censored_", tag_b),
paste0("TTFD10P_", tag_a),     paste0("TTFD10P_", tag_b),
paste0("censored10P_", tag_a), paste0("censored10P_", tag_b))
list(summary = summary, mismatch = mismatch)
}
# —— 按你当前环境中的对象名成对比较（把对象名替换成你环境里的）——
res_CF  <- compare_ttfd(TTFD_CF_19JUN2025,  TTFD_CF_22OCT2025,  "19JUN", "22OCT")
library(dplyr)
compare_ttfd <- function(a, b, tag_a = "A", tag_b = "B") {
x <- a %>%
select(UID, TTFD10, censored, TTFD10P, censored10P) %>%
rename_with(~paste0(., "_", tag_a), -UID) %>%
full_join(
b %>%
select(UID, TTFD10, censored, TTFD10P, censored10P) %>%
rename_with(~paste0(., "_", tag_b), -UID),
by = "UID"
)
# 预先拼好列名
c1 <- paste0("TTFD10_", tag_a);      c2 <- paste0("TTFD10_", tag_b)
c3 <- paste0("censored_", tag_a);    c4 <- paste0("censored_", tag_b)
c5 <- paste0("TTFD10P_", tag_a);     c6 <- paste0("TTFD10P_", tag_b)
c7 <- paste0("censored10P_", tag_a); c8 <- paste0("censored10P_", tag_b)
# 直接用 data.frame 索引计算，不依赖 .data
x$eq_TTFD10       <- (x[[c1]] == x[[c2]]) | (is.na(x[[c1]]) & is.na(x[[c2]]))
x$eq_censored     <- (x[[c3]] == x[[c4]]) | (is.na(x[[c3]]) & is.na(x[[c4]]))
x$eq_TTFD10P      <- (x[[c5]] == x[[c6]]) | (is.na(x[[c5]]) & is.na(x[[c6]]))
x$eq_censored10P  <- (x[[c7]] == x[[c8]]) | (is.na(x[[c7]]) & is.na(x[[c8]]))
summary <- x %>%
summarise(
n_UID = n(),
TTFD10_match      = mean(eq_TTFD10,      na.rm = TRUE) * 100,
censored_match    = mean(eq_censored,    na.rm = TRUE) * 100,
TTFD10P_match     = mean(eq_TTFD10P,     na.rm = TRUE) * 100,
censored10P_match = mean(eq_censored10P, na.rm = TRUE) * 100
)
mismatch <- x %>%
filter(!eq_TTFD10 | !eq_censored | !eq_TTFD10P | !eq_censored10P) %>%
select(UID, all_of(c1), all_of(c2), all_of(c3), all_of(c4),
all_of(c5), all_of(c6), all_of(c7), all_of(c8))
list(summary = summary, mismatch = mismatch)
}
res_CF  <- compare_ttfd(TTFD_CF_19JUN2025,  TTFD_CF_22OCT2025,  "19JUN", "22OCT")
table(TTFD_CF_19JUN2025$censored)
table(TTFD_CF_22OCT2025$censored)
table(TTFD_EF_22OCT2025$censored)
table(TTFD_EF_19JUNE2025$censored)
table(TTFD_EF_19JUN2025$censored)
View(TTFD_EF_19JUN2025)
View(TTFD_EF_22OCT2025)
library(dplyr)
compare_ttfd10 <- function(a, b, tag_a = "A", tag_b = "B") {
x <- a %>%
select(UID, TTFD10) %>%
rename_with(~paste0(., "_", tag_a), -UID) %>%
full_join(
b %>%
select(UID, TTFD10) %>%
rename_with(~paste0(., "_", tag_b), -UID),
by = "UID"
)
# 一致性判断
c1 <- paste0("TTFD10_", tag_a)
c2 <- paste0("TTFD10_", tag_b)
x$eq_TTFD10 <- (x[[c1]] == x[[c2]]) | (is.na(x[[c1]]) & is.na(x[[c2]]))
# 汇总匹配率
summary <- x %>%
summarise(
n_UID = n(),
TTFD10_match_rate = mean(eq_TTFD10, na.rm = TRUE) * 100
)
# 不一致UID明细
mismatch <- x %>%
filter(!eq_TTFD10) %>%
select(UID, all_of(c1), all_of(c2))
list(summary = summary, mismatch = mismatch)
}
res_CF  <- compare_ttfd10(TTFD_CF_19JUN2025,  TTFD_CF_22OCT2025,  "19JUN", "22OCT")
library(dplyr)
compare_ttfd10 <- function(a, b, tag_a = "A", tag_b = "B") {
x <- a %>%
select(UID, TTFD10) %>%
rename_with(~paste0(., "_", tag_a), -UID) %>%
full_join(
b %>%
select(UID, TTFD10) %>%
rename_with(~paste0(., "_", tag_b), -UID),
by = "UID"
)
c1 <- paste0("TTFD10_", tag_a)
c2 <- paste0("TTFD10_", tag_b)
x$eq_TTFD10 <- (x[[c1]] == x[[c2]]) | (is.na(x[[c1]]) & is.na(x[[c2]]))
# 汇总（避免 n()，直接用 nrow）
summary <- tibble(
n_UID = nrow(x),
TTFD10_match_rate = mean(x$eq_TTFD10, na.rm = TRUE) * 100,
# 额外给一个“真正值变了”的计数（两边都非NA且不相等）
changed_nonNA = sum(!is.na(x[[c1]]) & !is.na(x[[c2]]) & x[[c1]] != x[[c2]])
)
mismatch <- x %>%
filter(!eq_TTFD10) %>%
select(UID, all_of(c1), all_of(c2))
list(summary = summary, mismatch = mismatch)
}
res_CF  <- compare_ttfd10(TTFD_CF_19JUN2025,  TTFD_CF_22OCT2025,  "19JUN", "22OCT")
View(res_CF)
res_CF  <- compare_ttfd10(TTFD_CF_19JUN2025,  TTFD_CF_22OCT2025,  "19JUN", "22OCT")
res_EF  <- compare_ttfd10(TTFD_EF_19JUN2025,  TTFD_EF_22OCT2025,  "19JUN", "22OCT")
res_PF2 <- compare_ttfd10(TTFD_PF2_19JUN2025, TTFD_PF2_22OCT2025, "19JUN", "22OCT")
res_RF2 <- compare_ttfd10(TTFD_RF2_19JUN2025, TTFD_RF2_22OCT2025, "19JUN", "22OCT")
res_SF  <- compare_ttfd10(TTFD_SF_24SEP2025,  TTFD_SF_22OCT2025,  "24SEP", "22OCT")
bind_rows(
cbind(Scale = "CF",  res_CF$summary),
cbind(Scale = "EF",  res_EF$summary),
cbind(Scale = "PF2", res_PF2$summary),
cbind(Scale = "RF2", res_RF2$summary),
cbind(Scale = "SF",  res_SF$summary)
)
library(tidyverse)
library(lme4)      # for lmer/glmer
library(nlme)      # for lme
library(MASS)      # for polr, glm.nb
library(broom)     # for tidy() outputs
library(geepack)   # for GEE (if needed for comparison)
#| label: setup
# Load required packages
suppressPackageStartupMessages({
library(tidyverse)
library(lme4)      # for lmer/glmer
library(nlme)      # for lme
library(MASS)      # for polr, glm.nb
library(broom)     # for tidy() outputs
library(geepack)   # for GEE (if needed for comparison)
})
## IMPORTANT ####################################################
seed <-   # set this equal to the last 5 PID digits of your PID
#################################################################
stopifnot(seed >= 10000 & seed <= 99999)
#| label: setup
# Load required packages
suppressPackageStartupMessages({
library(tidyverse)
library(lme4)      # for lmer/glmer
library(nlme)      # for lme
library(MASS)      # for polr, glm.nb
library(broom)     # for tidy() outputs
library(geepack)   # for GEE (if needed for comparison)
})
## IMPORTANT ####################################################
seed <- 89128  # set this equal to the last 5 PID digits of your PID
#################################################################
stopifnot(seed >= 10000 & seed <= 99999)
set.seed(seed)
# Set options for clean output
options(contrasts = c("contr.treatment", "contr.poly"))
knitr::opts_chunk$set(fig.width = 6, fig.height = 4, dpi = 120)
#| label: data-simulation
message("Simulating dataset...")
N <- 120; Tt <- 5
id <- rep(1:N, each = Tt)
time <- rep(0:(Tt-1), N)
trt <- rep(rbinom(N, 1, 0.5), each = Tt)
# Random intercepts/slopes
b0 <- rep(rnorm(N, 0, 1.2), each = Tt)
b1 <- rep(rnorm(N, 0, 0.4), each = Tt)
# AR(1) errors
rho <- 0.5; sd_eps <- 1.0
eps <- unlist(lapply(1:N, function(i) {
as.numeric(arima.sim(list(ar = rho), n = Tt, sd = sd_eps))
}))
# Continuous response (with interaction)
y <- 10 + 0.6*time + 1.0*trt + 0.5*trt*time + b0 + b1*time + eps
# Binary outcome (main effects only for simplicity)
lp_bin <- -1 + 0.2*time + 0.6*trt
p_bin  <- plogis(lp_bin)
y_bin  <- rbinom(N*Tt, 1, p_bin)
# Count outcome (main effects only)
mu_count <- exp(1.2 + 0.15*time + 0.25*trt)
y_count  <- rpois(N*Tt, mu_count)
# Ordinal outcome (derived from y, so it also has an interaction)
cuts <- quantile(y, probs = c(.25, .5, .75))
ordinal_y <- cut(y, breaks = c(-Inf, cuts, Inf),
labels = c("L","M","H","VH"), ordered_result = TRUE)
dat <- tibble(
id = factor(id),
time = as.integer(time),
trt = factor(trt, levels = c(0,1), labels = c("Ctl","Tx")),
y = y,
y_bin = as.integer(y_bin),
y_count = as.integer(round(y_count)),
ordinal_y = ordered(ordinal_y)
)
glimpse(dat)
summary(dat[, c("y","y_bin","y_count","ordinal_y")])
#| label: lmm-fit
# Fit model and print summary
m_lme <- lme(
y ~ time * trt,
random = ~ time | id,     # 随机截距 + 随机斜率
data   = dat,
method = "REML",
na.action = na.omit
)
summary(m_lme)
# report variance components
vc <- VarCorr(m_lme)
vc
sigma_eps <- m_lme$sigma
sigma_eps
#| label: lmm-fit
# Fit model and print summary
# report variance components
#| label: lmm-fit
# Fit model and print summary
m_lme <- lme(
y ~ time * trt,
random = ~ time | id,
data   = dat,
method = "REML",
na.action = na.omit
)
summary(m_lme)
# report variance components
VarCorr(m_lme)
#| label: lmm-diagnostics
# obtain conditional residuals and fitted values
resid_c <- resid(m_lme, type = "normalized")
fitted_c <- fitted(m_lme)
# plot conditional residuals vs fitted values
plot(fitted_c, resid_c,
xlab = "Fitted values", ylab = "Conditional residuals",
main = "Residuals vs Fitted Values")
abline(h = 0, lty = 2, col = "gray")
# make qq plot for the conditional residuals
qqnorm(resid_c, main = "QQ Plot of Conditional Residuals")
qqline(resid_c, col = "red")
# create qq plots for the random effects (intercepts, and also for slopes)
re_df <- ranef(m_lme)
qqnorm(re_df[,"(Intercept)"], main = "QQ Plot: Random Intercepts")
qqline(re_df[,"(Intercept)"], col = "red")
qqnorm(re_df[,"time"], main = "QQ Plot: Random Slopes")
qqline(re_df[,"time"], col = "red")
#| label: logit-diagnostics
# plot the deviance residuals vs. the fitted probabilities
plot(fitted_probs, deviance_resid,
xlab = "Fitted probabilities",
ylab = "Deviance residuals",
main = "Deviance Residuals vs Fitted Probabilities")
#| label: logit-fit
# fit the model and print summary
glm_fit <- glm(
y_bin ~ time * trt,
data = dat,
family = binomial(link = "logit")
)
summary(glm_fit)
# Report the odds ratio based on the fitted model
exp_coef <- exp(coef(glm_fit))
exp_ci <- exp(confint(glm_fit))
odds_ratio <- data.frame(Estimate = exp_coef, exp_ci)
odds_ratio
#| label: logit-diagnostics
# plot the deviance residuals vs. the fitted probabilities
plot(fitted_probs, deviance_resid,
xlab = "Fitted probabilities",
ylab = "Deviance residuals",
main = "Deviance Residuals vs Fitted Probabilities")
#| label: logit-diagnostics
# plot the deviance residuals vs. the fitted probabilities
deviance_resid <- residuals(glm_fit, type = "deviance")
fitted_probs <- fitted(glm_fit)
plot(fitted_probs, deviance_resid,
xlab = "Fitted probabilities",
ylab = "Deviance residuals",
main = "Deviance Residuals vs Fitted Probabilities")
abline(h = 0, lty = 2, col = "gray")
#| label: count-models
# fit poisson model
pois_mod <- glm(y_count ~ time + trt, family = poisson, data = dat)
# fit quasi-poisson model
qpois_mod <- glm(y_count ~ time + trt, family = quasipoisson, data = dat)
# print output from tidy() on each model object
tidy(pois_mod)
tidy(qpois_mod)
# report the estimated dispersion parameter from the quasi-poisson model
disp_qp <- summary(qpois_mod)$dispersion
disp_qp
#| label: ordinal-fit
# fit the nodel and present the summary
ord_mod <- polr(ordinal_y ~ time * trt, data = your_data, Hess = TRUE)
#| label: ordinal-fit
# fit the nodel and present the summary
ord_mod <- polr(ordinal_y ~ time * trt, data = dat, Hess = TRUE)
summary(ord_mod)
exp(coef(ord_mod))  # odds ratio
#| label: ordinal-diagnostics
# Quick empirical cumulative logit plot for time
library(ggplot2)
K <- nlevels(dat$ordinal_y)
levs <- levels(dat$ordinal_y)
tmp <- do.call(rbind, lapply(1:(K-1), function(j){
p <- tapply(as.numeric(dat$ordinal_y) <= j, dat$time, mean)
data.frame(time = as.numeric(names(p)),
logit = log(p / (1 - p)),
threshold = levs[j])
}))
ggplot(tmp, aes(x = time, y = logit, color = threshold)) +
geom_point() + geom_smooth(method = "lm", se = FALSE) +
labs(title = "Empirical cumulative logit vs time",
y = "logit(P(Y ≤ threshold))") +
theme_minimal()
#| label: setup
# Load required packages
suppressPackageStartupMessages({
library(tidyverse)
library(lme4)      # for lmer/glmer
library(nlme)      # for lme
library(MASS)      # for polr, glm.nb
library(broom)     # for tidy() outputs
library(geepack)   # for GEE (if needed for comparison)
})
## IMPORTANT ####################################################
seed <- 89128  # set this equal to the last 5 PID digits of your PID
#################################################################
stopifnot(seed >= 10000 & seed <= 99999)
set.seed(seed)
# Set options for clean output
options(contrasts = c("contr.treatment", "contr.poly"))
knitr::opts_chunk$set(fig.width = 6, fig.height = 4, dpi = 120)
#| label: data-simulation
message("Simulating dataset...")
N <- 120; Tt <- 5
id <- rep(1:N, each = Tt)
time <- rep(0:(Tt-1), N)
trt <- rep(rbinom(N, 1, 0.5), each = Tt)
# Random intercepts/slopes
b0 <- rep(rnorm(N, 0, 1.2), each = Tt)
b1 <- rep(rnorm(N, 0, 0.4), each = Tt)
# AR(1) errors
rho <- 0.5; sd_eps <- 1.0
eps <- unlist(lapply(1:N, function(i) {
as.numeric(arima.sim(list(ar = rho), n = Tt, sd = sd_eps))
}))
# Continuous response (with interaction)
y <- 10 + 0.6*time + 1.0*trt + 0.5*trt*time + b0 + b1*time + eps
# Binary outcome (main effects only for simplicity)
lp_bin <- -1 + 0.2*time + 0.6*trt
p_bin  <- plogis(lp_bin)
y_bin  <- rbinom(N*Tt, 1, p_bin)
# Count outcome (main effects only)
mu_count <- exp(1.2 + 0.15*time + 0.25*trt)
y_count  <- rpois(N*Tt, mu_count)
# Ordinal outcome (derived from y, so it also has an interaction)
cuts <- quantile(y, probs = c(.25, .5, .75))
ordinal_y <- cut(y, breaks = c(-Inf, cuts, Inf),
labels = c("L","M","H","VH"), ordered_result = TRUE)
dat <- tibble(
id = factor(id),
time = as.integer(time),
trt = factor(trt, levels = c(0,1), labels = c("Ctl","Tx")),
y = y,
y_bin = as.integer(y_bin),
y_count = as.integer(round(y_count)),
ordinal_y = ordered(ordinal_y)
)
glimpse(dat)
summary(dat[, c("y","y_bin","y_count","ordinal_y")])
#| label: logit-fit
# fit the model and print summary
glm_fit <- glm(
y_bin ~ time * trt,
data = dat,
family = binomial(link = "logit")
)
summary(glm_fit)
# Report the odds ratio based on the fitted model
exp_coef <- exp(coef(glm_fit))
exp_ci <- exp(confint(glm_fit))
odds_ratio <- data.frame(Estimate = exp_coef, exp_ci)
odds_ratio
#| label: ordinal-fit
# fit the nodel and present the summary
ord_mod <- polr(ordinal_y ~ time * trt, data = dat, Hess = TRUE)
summary(ord_mod)
exp(coef(ord_mod))  # odds ratio
#| label: ordinal-diagnostics
# Quick empirical cumulative logit plot for time
library(ggplot2)
K <- nlevels(dat$ordinal_y)
levs <- levels(dat$ordinal_y)
tmp <- do.call(rbind, lapply(1:(K-1), function(j){
p <- tapply(as.numeric(dat$ordinal_y) <= j, dat$time, mean)
data.frame(time = as.numeric(names(p)),
logit = log(p / (1 - p)),
threshold = levs[j])
}))
ggplot(tmp, aes(x = time, y = logit, color = threshold)) +
geom_point() + geom_smooth(method = "lm", se = FALSE) +
labs(title = "Empirical cumulative logit vs time",
y = "logit(P(Y ≤ threshold))") +
theme_minimal()
